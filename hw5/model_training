import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Loading the dataset
df = pd.read_csv('pima-indians-diabetes.csv')

# Separating the features and the target variables
x = df.drop('HasDiabetes', axis = 1).values
y = df['HasDiabetes'].values

# Splitting dataset into train and test sets
x_train, x_test = x[:500], x[500:]
y_train, y_test = y[:500], y[500:]

#print(x_train[:5])

# Min-max Normalization
x_min = x_train.min(axis = 0)
x_max = x_train.max(axis = 0)
x_train = (x_train - x_min) / (x_max - x_min)
x_test = (x_test - x_min) / (x_max - x_min)

def sgd_update_linear(w, bias, x, y, learning_rate):
    # Linear regression
    y_pred = np.dot(x, w) + bias
    
    # Update w and bias
    error = y_pred - y
    w -= learning_rate * error * x
    bias -= learning_rate * error
    
    return w, bias

def sgd_update_logistic(w, bias, x, y, learning_rate):
    # Logisitic regression
    y_pred = 1 / (1 + np.exp(-np.dot(x, w) - bias))
    
    # Update w and bias
    error = y_pred - y
    w -= learning_rate * error * x
    bias -= learning_rate * error
    
    return w, bias

# I think this is MSE and needs to be SSE 
def sse_loss(y, y_pred):
    return ((y - y_pred) ** 2)

def logistic_predict(w, bias, x):
    return 1 / (1 + np.exp(-np.dot(x, w) - bias)) > 0.5

def linear_predict(w, bias, x):
    return np.dot(x, w) + bias


def train_sgd(x, y, learning_rate, epochs, model_type):
    w = np.zeros(x.shape[1])
    bias = 0 
    loss = 0
    avg_losses = []
    
    for epoch in range(epochs):
        for i in range(len(x)):
            if model_type == 'linear':
                w, bias = sgd_update_linear(w, bias, x[i], y[i], learning_rate)
            elif model_type == 'logistic':
                w, bias = sgd_update_logistic(w, bias, x[i], y[i], learning_rate)
                
            y_pred = logistic_predict(w, bias, x[i]) if model_type == 'logistic' else linear_predict(w, bias, x[i])
            loss += sse_loss(y[i], y_pred)
            avg_loss = loss / (i + 1)
             
            if (i + 1) % 100 == 0:
                avg_losses.append(avg_loss)
                
                # Print the loss every 100 iterations
                print(f'Model: {model_type}, Learning Rate: {learning_rate}, Step {i + 1}, Loss: {avg_loss}')
    
    l2_norm = np.linalg.norm(w, ord = 2)
    print(f'L2 Norm of weights: {l2_norm}\n')
    return w, bias, avg_losses


# Run Tests
def test_model(w, bias, x, y, model_type):
    loss = 0
    avg_losses = []
    
    for i in range(len(x)):
        y_pred = logistic_predict(w, bias, x[i]) if model_type == 'logistic' else linear_predict(w, bias, x[i])
        loss += sse_loss(y[i], y_pred)
        avg_loss = loss / (i + 1)
        
        if (i + 1) % 100 == 0:
            avg_losses.append(avg_loss)
            print(f'\nModel: {model_type}, Step: {i + 1}, Average Loss: {avg_loss}, SSE: {loss}')
        elif (i + 1) == 268:
            print(f'\nModel: {model_type}, Step: {i + 1}, Average Loss: {avg_loss}, SSE: {loss}')


learning_rate = [0.8, 0.001, 0.00001]
epochs = 1

for i in learning_rate:
    # Train Linear Regression Model
    w_linear, bias_linear, losses_linear = train_sgd(x_train, y_train, i, epochs, 'linear')

    # Train Logistic Regression Model
    w_logistic, bias_logistic, losses_logistic = train_sgd(x_train, y_train, i, epochs, 'logistic')

    # Plot the Losses
    plt.plot(losses_linear, label = 'Linear Regression')
    plt.plot(losses_logistic, label = 'Logistic Regression')
    plt.xlabel('Steps')
    plt.ylabel('Average Loss')
    plt.title(f'Average Losses when Learning Rate = {i}')
    plt.legend()
    plt.savefig(f'losses_learning_rate_{i}.png')


# Test the models
test_model(w_linear, bias_linear, x_test, y_test, 'linear')
test_model(w_logistic, bias_logistic, x_test, y_test, 'logistic')
